{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Fundamentals of Bayesian Inference\n",
    "\n",
    "Bayesian inference is the process of fitting a probability model to a set of data and sumarizing the result by a probability distribution on the parameters of the model and on unobserved quantities such as predictions for new observations. In chapters 1-3, we introduce several useful families of models and illustrate their application in the analysis of relatively simple data structures. Some mathematics arises in the analytical manipulation of the probability distributions, notably in transformation and integration in multiparameter problems. We differ somewhat from other introductions to bayesian inference by emphasizing stochastic simulation, and the combination of mathematical analysis and simulation, as general methods for summarizing distributions. chapter 4 outlines the fundamental connections between bayesian and other approaches to statistical inference. The early chapters focus on simple example to develop the basic ideas of bayesian inference; examples in which the Bayesian approach makes a practical difference relative to more traditional approaches begin to appear in chapter 3. The major practical advantages of the Bayesian approach appear in chapter 5, where we introduce hierarchical models, which allow the parameters of a prior, or pupulation, distribution themselves to be estimated from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The three steps of bayesian data analysis\n",
    "\n",
    "The process of Bayesian data analysis can be idealized by dividing it into the following three steps:\n",
    "\n",
    "1. setting up a full probability model- a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\n",
    "\n",
    "2. Conditioning on observed data: calculating and interpreting the appropriate posterior distribution- the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data\n",
    "\n",
    "3. Evaluating the fir of the model and the implications of the resulting posterior distribution: how well does the model fir the data, are the substative conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? in response one can alter or expand the model and repeat the three steps\n",
    "\n",
    "A primary motivation for Bayesian thinking is that it facilitates a common-sense interpretation of statistical conclusions. For instance, a Bayesian probability interval for an unknown quantity of interest can be directly regarded as having a high probablity of containing the unkown quantity, in contrast to a frequentist interval, which may strictly be interpreted only in relation to a sequence of similar inference that might be made in repeated practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 General notation for statistical inference\n",
    "\n",
    "Statistical inference in concerned with drawing conclusions, from numerical data, about quantities that are not observed. For example, a clinical trial of a new cancer drug might be designed to compare the five-year survival probability in a population given the new drug that in a population under standard treatment. These survival probabilities refer to a large population of patients and it is neither feasible nor ethically acceptable to experiment on an entire population. therefor inference about the true probabilities and, in particular, their differences must be based on a sample of patients.\n",
    "\n",
    "We distinguish between two kinds of estimands: unobserved quentatites for which statistical inference are made_ first, potentially obervable quantities, such as future observation of a process, or the outcome under the treatment not recieved in the clinical trial example; and second quantities that re not directly observable, that is, parameters that govern the hypothetical process leading to the observed data. the distinction between these two kinds of estimands is not always precise, but is generally useful as a way of understanding how a statistical model for a particular problem first into the real world.\n",
    "\n",
    "<b>Parameters, data, and predictions</b>\n",
    "\n",
    "As general notation, we let θ denote unobservable vector quantities or population parameters of interest (such as the probabilities of survival under each treatment for randomly chosen members of the population in the example of the clinical trial), y denote the observed data (such as the numbers of surviros and deaths in each treatment group), and ˜y denote unkown, but potentially obervable quantities (such as the outcomes of the patients under the other treatment, or the outcome under each of the treatments for a new patient similar to those already in the trial). Generally use greek letters for parameters, lower case roman letters for observed or observable scalars and vectors, and uper case roman letters for observer or observable matrices.\n",
    "\n",
    "<b>Observational units variables</b>\n",
    "\n",
    "In many studies data are gathered on each of a set of N objects or units, and we can write the data s a vector, y(y1, ..., yn).\n",
    "\n",
    "<b>Exhangeability</b>\n",
    "\n",
    "The usual starting point of a statistical analysis is the assumption that the n values yi may be regarded as exhangeable, meaning that we express uncertainty as joint probability density that is invariant to permutations of the indexes. A nonexchangeable model would be approariate if information relevant to the outcome were conveyed in the unit indexes rather than by explanatory variables. The idea of exhangeability is fundamental to statistics, and we return to it repeatedly throughtout the book.\n",
    "\n",
    "<b>Explanatory variables</b>\n",
    "\n",
    "It is common to have obervations on each unit that we do not bother to model as random such as age and previous health status. We call this second class of variables explanatory variables, or covariates, and label them x. we use X to denote the entire set of explanatory variables. will be fully explained in detail in chapter 8\n",
    "\n",
    "<b>Hierarchical modeling</b>\n",
    "\n",
    "Hierarchical models are used when information is available on several different levels of observational units. will be discussed in chapter 5 and subsequent chapters. It is possible to speak of exhangability at each level of units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bayesian inference\n",
    "\n",
    "Bayesian statistical conclusions about parameter θ, are made in terms of probability statements. these probability statementsare conditional on the observed value of y, and in our notation are written sumple as p(θ|y). We also implicitly condition on the known values of any covariates, x. It is at the fundamental level of conditioning on oberved dat that bayesian inference departs from the approach to statistical inference described in many textbooks, which is based on a retrospective evaluation of the precedure used to estimate θ over the distribution of possible y values conditional on the true unkown value of θ.\n",
    "\n",
    "<b>Probability notation</b>\n",
    "\n",
    "1. p(.|.) denoted a conditional probability density with the arguments determined by the context\n",
    "2. p(.) denotes a marginal distribution\n",
    "\n",
    "<b>Bayes'Rule</b>\n",
    "\n",
    "In order to make probability statements about θ giveny, we must begin with a model providing a joint probability distribution for θ and y. the joint probability mass or density function can be written as a product of two densities that are often referred to as the prior distribution p(θ) and the sampling distribution p(y|θ) \n",
    "\n",
    "Simply conditioning on the unknown value of the data y using the propery of conditional probability known as Bayes'rule yields the posterior density:\n",
    "\n",
    "p(θ|y) = p(θ)p(y|θ) / p(y)\n",
    "\n",
    "where p(y) = the sum of p(θ)p(y|θ), and the sum over all possible values of θ. In case we deal with continuous data it is the integral instead of the sum.\n",
    "\n",
    "<b>prediction</b>\n",
    "\n",
    "To make inferences about an unkown observable, often called predictive inferences, we follow a similar logic. Before the data y are considered, the distribution of the unkown but obserbale y is\n",
    "\n",
    "p(y) = Zp(y, θ)dθ =Zp(θ)p(y|θ)dθ\n",
    "\n",
    "where Z is used as an integral.\n",
    "\n",
    "This is often called the marginal dsitribution of y, but a more informative name is the prior predictive distribution: prior because it is not conditional on a previous observation of the process, and predictive because it is the distribution ofr a quantity that is observable, from the same process.\n",
    "\n",
    "<b>likelihood</b>\n",
    "\n",
    "Using Bayes'rule with a chosen probability model means that the data y affect the posterior inference only throught p(y|θ) which, when regarded as a function of θ, for fixed y, is called the likelihood function. In this way bayesian inference is obeying what is sometimes called likelihood principle.\n",
    "\n",
    "<b>Likelihood and odds ratios</b>\n",
    "\n",
    "The ratio of the posterior density p(θ|y) evaluated at the points θ1 and θ2 under a given model is called the posterior odds for θ1 compared to θ2. the most familiar application of this concept with discrete parameters with θ2 taken to be the complement of θ1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Discrete examples: genetics and spell checking\n",
    "\n",
    "This is an example to demonstrate Bayes'Theorem in which the immediate goal is enference about a particular discrete quantity rather than with estimation of a parameter that describes an entire population. These discrete examples allow us to see the prior, likelihood, and posterior probabilities directly.\n",
    "\n",
    "<b>inference about a genetic status</b>\n",
    "\n",
    "males have one X and one Y chromosome whereas females have two X-chromosomes, each chromosome being inherited from one parent. Hemophilia is a disease that exhibits X chromosome linked recessive inheritance, meaning that a male who inherits the  gene that causes the disease on the X chromosome is affected whereas a female carrying the gene on only of her two X chromosomes is not affected. The disease is generally fetal for woman who inherit two such genes, and this is rare, since the frequency of occurrence of the gene is low in human populations.\n",
    "\n",
    "PRIOR DISTRIBUTION. Consider a woman who has an affected borther, which implies that her mother must be carrier of the hemophilia gene with one good and one bad hemophilia. We are also told that her father is not affected; thus the woman herself has a 50-50 chance of having the gene. The unkown quantity of interest, the state of the woman, has just two values: the woman is either a carrier of the gene (θ = 1) or not (θ = 0). based on the information provided thus far, the prior distribution for the unkown θ can be expressed simplay as Pr(θ =1) = Pr(θ = 0)= 1/2\n",
    "\n",
    "DATA MODEL AND LIKELIHOOD. The data used to update the prior information consist of the affection status of the woman's suppose she has two sons, neither of whom is affected. let yi = 1 or 0 denote an affected or unaffected son, respectively. The outcomes of the two sons are exhangeable and conditional on the unkown θ, are independent; we assume the sons are not identical twin. The items of independent data generate the following likelihood function:\n",
    "\n",
    "Pr(y1 = 0, y2=0 | θ = 1) = (0.5)(0.5) = 0.25\n",
    "Pr(y1 = 0, y2 = 0 | θ= 0) = (1)(1) = 1\n",
    "\n",
    "These expressions follow from the fact that if the woman is a carrier, then each of her sons willhave a 50% chance of inheriting the gene and so being affected wheareas if she is not a carrier then the probability is close to 1 that a son of hers will be unaffected.\n",
    "\n",
    "POSTERIOR DISTRIBUTION. Bayes'rule can now be used to combine the information in the data with the prior probability; in particular, interest is likely to focus on the posterior probability that the woman is a carrier. using y to denote the joint data, this is simply\n",
    "\n",
    "Pr(θ = 1|y) = p(y|θ = 1)Pr(θ = 1) / p(y|θ = 1)Pr(θ = 1) + p(y|θ = 0)Pr(θ = 0)\n",
    "= (0.25)(0.5) / (0.25)(0.5) + (1.0)(0.5) \n",
    "=0.125 / 0.625 = 0.20.\n",
    "\n",
    "Intuitively it is clear that if a woman has unaffected children it is less probable that she is a carrier, and bayes'rule provides a formal mechanism for determining the extend of the correction. The results can also be described in terms of prior and posterior odds. The prior odds of the woman being a carrier are 0.5/0.5 = 1. the likelihood ratio based on the information about her two unaffecton sons is 0.25, so the posterior odds are 1*0.25 = 0.25 converting back to a probability, we obtain(0.25 / (1+0.25)) = 0.20, as before.\n",
    "\n",
    "ADDING MORE DATA. A key aspect of Bayesian analysis is the ease with which sequential analyses can be performed. for example, suppose that the woman has a third son, who is also unaffected. The entire calculation does not need to be redone; rather we use the previous posterior distribution as the new prior distribution to obtain.\n",
    "\n",
    "Pr(θ = 1|y1, y2, y3) = (0.5)(0.20) / (0.5)(0.20) + (1)(0.8) = 0.111\n",
    "\n",
    "Alternatively, if we suppose that that the third son is affected, it is easy to check that the posterior probability of the woman being a carrier becomes 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Probability as a measure of uncertainty\n",
    "\n",
    "In bayesian statistics, probability is used as the fundamental measure of uncertainty. Within this paradigm, it is equally legitimate to discuss the probability of rain tomorow or a brazilian victory in the soccer world cup as it is to discuss the probability that a coin toss will land heads. Bayesian methods enable statements to be made about the partial knowledge available (based on data) concerning some situation or state of naturein a systematic way using probability as the fundamental measure. The guiding principle is that the state of knowledge about unkown is described by a probability distribution.\n",
    "\n",
    "What is meant by numberical measure of uncertainty? form example the probability of heads is widely agreeed to be 1/2 why is this so? two justification seem to be commonly given \n",
    "\n",
    "1. Symmetry or exhangeability argument:\n",
    "probability = number of favorable cases / number of possibilities. assuming equally likely possibilities. for a coins toss this is really a physicalargumant based on assumptions about the forces at work in determining the manner in which the coin will faill.\n",
    "\n",
    "2. Frequency argument: probability = relatice frequency obtained in along  sequence of tosses, assumed to be performed in a identical manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Example probabilities from football point spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Example: calibration for record linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Some useful results from probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Computation and software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Bayesian inference in applied statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

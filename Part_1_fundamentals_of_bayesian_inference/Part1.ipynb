{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Fundamentals of Bayesian Inference\n",
    "\n",
    "Bayesian inference is the process of fitting a probability model to a set of data and sumarizing the result by a probability distribution on the parameters of the model and on unobserved quantities such as predictions for new observations. In chapters 1-3, we introduce several useful families of models and illustrate their application in the analysis of relatively simple data structures. Some mathematics arises in the analytical manipulation of the probability distributions, notably in transformation and integration in multiparameter problems. We differ somewhat from other introductions to bayesian inference by emphasizing stochastic simulation, and the combination of mathematical analysis and simulation, as general methods for summarizing distributions. chapter 4 outlines the fundamental connections between bayesian and other approaches to statistical inference. The early chapters focus on simple example to develop the basic ideas of bayesian inference; examples in which the Bayesian approach makes a practical difference relative to more traditional approaches begin to appear in chapter 3. The major practical advantages of the Bayesian approach appear in chapter 5, where we introduce hierarchical models, which allow the parameters of a prior, or pupulation, distribution themselves to be estimated from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The three steps of bayesian data analysis\n",
    "\n",
    "The process of Bayesian data analysis can be idealized by dividing it into the following three steps:\n",
    "\n",
    "1. setting up a full probability model- a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.\n",
    "\n",
    "2. Conditioning on observed data: calculating and interpreting the appropriate posterior distribution- the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data\n",
    "\n",
    "3. Evaluating the fir of the model and the implications of the resulting posterior distribution: how well does the model fir the data, are the substative conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? in response one can alter or expand the model and repeat the three steps\n",
    "\n",
    "A primary motivation for Bayesian thinking is that it facilitates a common-sense interpretation of statistical conclusions. For instance, a Bayesian probability interval for an unknown quantity of interest can be directly regarded as having a high probablity of containing the unkown quantity, in contrast to a frequentist interval, which may strictly be interpreted only in relation to a sequence of similar inference that might be made in repeated practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 General notation for statistical inference\n",
    "\n",
    "Statistical inference in concerned with drawing conclusions, from numerical data, about quantities that are not observed. For example, a clinical trial of a new cancer drug might be designed to compare the five-year survival probability in a population given the new drug that in a population under standard treatment. These survival probabilities refer to a large population of patients and it is neither feasible nor ethically acceptable to experiment on an entire population. therefor inference about the true probabilities and, in particular, their differences must be based on a sample of patients.\n",
    "\n",
    "We distinguish between two kinds of estimands: unobserved quentatites for which statistical inference are made_ first, potentially obervable quantities, such as future observation of a process, or the outcome under the treatment not recieved in the clinical trial example; and second quantities that re not directly observable, that is, parameters that govern the hypothetical process leading to the observed data. the distinction between these two kinds of estimands is not always precise, but is generally useful as a way of understanding how a statistical model for a particular problem first into the real world.\n",
    "\n",
    "<b>Parameters, data, and predictions</b>\n",
    "\n",
    "As general notation, we let θ denote unobservable vector quantities or population parameters of interest (such as the probabilities of survival under each treatment for randomly chosen members of the population in the example of the clinical trial), y denote the observed data (such as the numbers of surviros and deaths in each treatment group), and ˜y denote unkown, but potentially obervable quantities (such as the outcomes of the patients under the other treatment, or the outcome under each of the treatments for a new patient similar to those already in the trial). Generally use greek letters for parameters, lower case roman letters for observed or observable scalars and vectors, and uper case roman letters for observer or observable matrices.\n",
    "\n",
    "<b>Observational units variables</b>\n",
    "\n",
    "In many studies data are gathered on each of a set of N objects or units, and we can write the data s a vector, y(y1, ..., yn).\n",
    "\n",
    "<b>Exhangeability</b>\n",
    "\n",
    "The usual starting point of a statistical analysis is the assumption that the n values yi may be regarded as exhangeable, meaning that we express uncertainty as joint probability density that is invariant to permutations of the indexes. A nonexchangeable model would be approariate if information relevant to the outcome were conveyed in the unit indexes rather than by explanatory variables. The idea of exhangeability is fundamental to statistics, and we return to it repeatedly throughtout the book.\n",
    "\n",
    "<b>Explanatory variables</b>\n",
    "\n",
    "It is common to have obervations on each unit that we do not bother to model as random such as age and previous health status. We call this second class of variables explanatory variables, or covariates, and label them x. we use X to denote the entire set of explanatory variables. will be fully explained in detail in chapter 8\n",
    "\n",
    "<b>Hierarchical modeling</b>\n",
    "\n",
    "Hierarchical models are used when information is available on several different levels of observational units. will be discussed in chapter 5 and subsequent chapters. It is possible to speak of exhangability at each level of units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Discrete examples: genetics and spell checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Probability as a measure of uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Example probabilities from football point spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Example: calibration for record linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Some useful results from probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Computation and software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Bayesian inference in applied statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
